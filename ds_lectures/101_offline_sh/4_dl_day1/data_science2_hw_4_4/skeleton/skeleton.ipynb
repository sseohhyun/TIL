{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ca4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3c947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터\n",
    "train_data = [\n",
    "    (\"I love this movie\", 1),\n",
    "    (\"This film was terrible\", 0),\n",
    "    (\"Absolutely fantastic!\", 1),\n",
    "    (\"I did not like this movie\", 0),\n",
    "    (\"What a great experience\", 1),\n",
    "    (\"The acting was horrible\", 0),\n",
    "    (\"I enjoyed every moment\", 1),\n",
    "    (\"The plot was boring\", 0),\n",
    "    (\"Brilliant performance by the actors\", 1),\n",
    "    (\"Worst movie I have ever seen\", 0),\n",
    "    (\"The soundtrack was amazing\", 1),\n",
    "    (\"I will never watch this again\", 0),\n",
    "]\n",
    "\n",
    "# 테스트 데이터 (훈련에 없던 단어도 일부 포함)\n",
    "test_data = [\n",
    "    (\"This movie is amazing\", 1),\n",
    "    (\"I hated the film\", 0),\n",
    "    (\"The story was wonderful\", 1),\n",
    "    (\"It was a waste of time\", 0),\n",
    "    (\"Such an excellent movie\", 1),\n",
    "    (\"The characters were disappointing\", 0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 정수 인코딩\n",
    "# Tokenizer는 문장을 단어 단위로 나누고, 각 단어를 고유 숫자로 매핑합니다.\n",
    "# oov_token=\"<OOV>\"는 학습되지 않은(처음 보는) 단어를 위한 기본 토큰입니다.\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV\")\n",
    "tokenizer.fit_on_texts([text for text, _ in train_data]) # 학습 데이터에서 어휘집 생성\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 어휘집 크기 (패딩 인덱스 용도를 위해 +1을 해줘야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train + test 전체 문장을 합쳐서 최대 길이 찾기\n",
    "all_texts = [t for t, _ in train_data + test_data]\n",
    "max_len = max(len(seq) for seq in tokenizer.texts_to_sequences(all_texts))\n",
    "\n",
    "# 텍스트를 정수 인덱스로 변환 후, 시퀀스 길이를 맞춰주는 함수입니다.\n",
    "# pad_sequences는 모든 문장의 길이를 동일하게 맞추기 위해 짧은 문장을 패딩합니다.\n",
    "def encode_and_pad(data, maxlen):\n",
    "    texts, labels = zip(*data)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "    return np.array(padded), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21baabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 및 테스트 데이터를 전처리합니다.\n",
    "X_train, y_train = encode_and_pad(train_data, max_len)\n",
    "X_test, y_test = encode_and_pad(test_data, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "embedding_dim = 100  # 각 단어를 100차원 벡터로 표현\n",
    "hidden_dim = 128     # RNN의 은닉 상태 크기\n",
    "output_dim = 2       # 출력 클래스 수 (긍정 vs 부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675d62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델 정의\n",
    "model = Sequential([\n",
    "    # 임베딩 레이어: 각 단어를 고정된 차원의 벡터로 변환합니다.\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X_train.shape[1]),\n",
    "    \n",
    "    # RNN 레이어: 순차적으로 들어오는 단어 벡터를 기반으로 문장의 흐름을 학습합니다.\n",
    "    SimpleRNN(units=hidden_dim),\n",
    "    \n",
    "    # 출력층: 2개의 노드 (긍정/부정)를 softmax 함수로 확률 출력\n",
    "    Dense(output_dim, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b3c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.7141\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8333 - loss: 0.6431\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9167 - loss: 0.5783\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 1.0000 - loss: 0.5147\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 1.0000 - loss: 0.4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c94d52bfd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 컴파일 및 학습\n",
    "# 손실 함수는 sparse_categorical_crossentropy: 정수형 레이블을 사용할 때 적합합니다.\n",
    "# 옵티마이저는 Adam으로 빠른 수렴을 기대할 수 있습니다.\n",
    "model.compile(optimizer='adam',\n",
    "        loss ='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "# fit 함수를 통해 모델을 학습합니다.\n",
    "# verbose=1은 학습 과정을 상세히 출력합니다.\n",
    "model.fit(X_train, y_train, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0b6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n",
      "\n",
      "Test Accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# predict 함수를 사용해 테스트 데이터를 예측합니다.\n",
    "# np.argmax를 이용해 softmax 결과에서 가장 높은 확률의 클래스를 선택합니다.\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# sklearn의 accuracy_score를 사용해 정확도를 측정합니다.\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nTest Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "성능 불일치 설명\n",
    "\n",
    "이 코드에서 loss는 줄어들지만, accuracy는 일관되지 않게 변할 수 있습니다.\n",
    "그 이유는:\n",
    "1. 작은 데이터셋 크기: 모델이 다양한 패턴을 학습하지 못합니다.\n",
    "2. 가중치 초기화: RNN 가중치가 매번 무작위로 초기화되어 결과가 다를 수 있습니다.\n",
    "3. 과적합: 모델이 훈련 데이터에 너무 맞춰져, 테스트 성능이 저하될 수 있습니다.\n",
    "\n",
    "이를 해결하려면 데이터셋을 확장하거나 정규화 기법을 적용할 수 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac61b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
